{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10091986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict_decoder = torch.load(\"checkpoint_decoder_finetune02010000.pth\", map_location='cpu', weights_only=False)\n",
    "state_dict_encoder = torch.load(\"checkpoint_encoder_finetune02010000.pth\", map_location='cpu', weights_only=False)\n",
    "colornet = torch.load(\"colornet_iter_342000.pth\", map_location='cpu', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c960bed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict_encoder: <class 'dict'>\n",
      "dict_keys(['model', 'inter_save', 'epoch', 'total_iter', 'args'])\n",
      "\n",
      "state_dict_decoder: <class 'dict'>\n",
      "dict_keys(['model', 'epoch', 'total_iter', 'args'])\n",
      "\n",
      "colornet: <class 'collections.OrderedDict'>\n",
      "odict_keys(['conv1_1.0.weight', 'conv1_1.0.bias', 'conv1_1.2.weight', 'conv1_1.2.bias', 'conv1_2.weight', 'conv1_2.bias', 'conv1_2norm_ss.weight', 'conv2_1.weight', 'conv2_1.bias', 'conv2_2.weight', 'conv2_2.bias', 'conv2_2norm_ss.weight', 'conv3_1.weight', 'conv3_1.bias', 'conv3_2.weight', 'conv3_2.bias', 'conv3_3.weight', 'conv3_3.bias', 'conv3_3norm_ss.weight', 'conv4_1.weight', 'conv4_1.bias', 'conv4_2.weight', 'conv4_2.bias', 'conv4_3.weight', 'conv4_3.bias', 'conv5_1.weight', 'conv5_1.bias', 'conv5_2.weight', 'conv5_2.bias', 'conv5_3.weight', 'conv5_3.bias', 'conv6_1.weight', 'conv6_1.bias', 'conv6_2.weight', 'conv6_2.bias', 'conv6_3.weight', 'conv6_3.bias', 'conv7_1.weight', 'conv7_1.bias', 'conv7_2.weight', 'conv7_2.bias', 'conv7_3.weight', 'conv7_3.bias', 'conv8_1.1.weight', 'conv8_1.1.bias', 'conv3_3_short.weight', 'conv3_3_short.bias', 'conv8_2.weight', 'conv8_2.bias', 'conv8_3.weight', 'conv8_3.bias', 'conv9_1.1.weight', 'conv9_1.1.bias', 'conv2_2_short.weight', 'conv2_2_short.bias', 'conv9_2.weight', 'conv9_2.bias', 'conv10_1.1.weight', 'conv10_1.1.bias', 'conv1_2_short.weight', 'conv1_2_short.bias', 'conv10_2.weight', 'conv10_2.bias', 'conv10_ab.weight', 'conv10_ab.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"state_dict_encoder:\",type(state_dict_encoder))  # <class 'dict'>\n",
    "print(state_dict_encoder.keys())  # 可能包含 'model', 'optimizer', 'epoch' 等   \n",
    "print()\n",
    "\n",
    "print(\"state_dict_decoder:\",type(state_dict_decoder))  # <class 'dict'>\n",
    "print(state_dict_decoder.keys())  # 可能包含 'model', 'optimizer', 'epoch' 等\n",
    "print()\n",
    "\n",
    "print(\"colornet:\",type(colornet))  # <class 'dict'>\n",
    "print(colornet.keys())  # 可能包含 'model', 'optimizer', 'epoch' 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b05b292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.encoder.layers.0.self_attn.in_proj_weight: torch.Size([1152, 384])\n",
      "transformer.encoder.layers.0.self_attn.in_proj_bias: torch.Size([1152])\n",
      "transformer.encoder.layers.0.self_attn.bias_k: torch.Size([1, 1, 384])\n",
      "transformer.encoder.layers.0.self_attn.bias_v: torch.Size([1, 1, 384])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.weight: torch.Size([384, 384])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.bias: torch.Size([384])\n",
      "transformer.encoder.layers.0.linear1.weight: torch.Size([2048, 384])\n",
      "transformer.encoder.layers.0.linear1.bias: torch.Size([2048])\n",
      "transformer.encoder.layers.0.linear2.weight: torch.Size([384, 2048])\n",
      "transformer.encoder.layers.0.linear2.bias: torch.Size([384])\n",
      "transformer.encoder.layers.0.conv.1.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.0.conv.1.bias: torch.Size([512])\n",
      "transformer.encoder.layers.0.conv.5.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.0.conv.5.bias: torch.Size([512])\n",
      "transformer.encoder.layers.0.conv.9.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.0.conv.9.bias: torch.Size([512])\n",
      "transformer.encoder.layers.0.fuse_cnn2trans.0.weight: torch.Size([384, 896])\n",
      "transformer.encoder.layers.0.fuse_cnn2trans.0.bias: torch.Size([384])\n",
      "transformer.encoder.layers.0.fuse_trans2cnn.0.weight: torch.Size([512, 896, 1, 1])\n",
      "transformer.encoder.layers.0.fuse_trans2cnn.0.bias: torch.Size([512])\n",
      "transformer.encoder.layers.0.norm1.weight: torch.Size([384])\n",
      "transformer.encoder.layers.0.norm1.bias: torch.Size([384])\n",
      "transformer.encoder.layers.0.norm2.weight: torch.Size([384])\n",
      "transformer.encoder.layers.0.norm2.bias: torch.Size([384])\n",
      "transformer.encoder.layers.1.self_attn.in_proj_weight: torch.Size([1152, 384])\n",
      "transformer.encoder.layers.1.self_attn.in_proj_bias: torch.Size([1152])\n",
      "transformer.encoder.layers.1.self_attn.bias_k: torch.Size([1, 1, 384])\n",
      "transformer.encoder.layers.1.self_attn.bias_v: torch.Size([1, 1, 384])\n",
      "transformer.encoder.layers.1.self_attn.out_proj.weight: torch.Size([384, 384])\n",
      "transformer.encoder.layers.1.self_attn.out_proj.bias: torch.Size([384])\n",
      "transformer.encoder.layers.1.linear1.weight: torch.Size([2048, 384])\n",
      "transformer.encoder.layers.1.linear1.bias: torch.Size([2048])\n",
      "transformer.encoder.layers.1.linear2.weight: torch.Size([384, 2048])\n",
      "transformer.encoder.layers.1.linear2.bias: torch.Size([384])\n",
      "transformer.encoder.layers.1.conv.1.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.1.conv.1.bias: torch.Size([512])\n",
      "transformer.encoder.layers.1.conv.5.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.1.conv.5.bias: torch.Size([512])\n",
      "transformer.encoder.layers.1.conv.9.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.1.conv.9.bias: torch.Size([512])\n",
      "transformer.encoder.layers.1.fuse_cnn2trans.0.weight: torch.Size([384, 896])\n",
      "transformer.encoder.layers.1.fuse_cnn2trans.0.bias: torch.Size([384])\n",
      "transformer.encoder.layers.1.fuse_trans2cnn.0.weight: torch.Size([512, 896, 1, 1])\n",
      "transformer.encoder.layers.1.fuse_trans2cnn.0.bias: torch.Size([512])\n",
      "transformer.encoder.layers.1.norm1.weight: torch.Size([384])\n",
      "transformer.encoder.layers.1.norm1.bias: torch.Size([384])\n",
      "transformer.encoder.layers.1.norm2.weight: torch.Size([384])\n",
      "transformer.encoder.layers.1.norm2.bias: torch.Size([384])\n",
      "transformer.encoder.layers.2.self_attn.in_proj_weight: torch.Size([1152, 384])\n",
      "transformer.encoder.layers.2.self_attn.in_proj_bias: torch.Size([1152])\n",
      "transformer.encoder.layers.2.self_attn.bias_k: torch.Size([1, 1, 384])\n",
      "transformer.encoder.layers.2.self_attn.bias_v: torch.Size([1, 1, 384])\n",
      "transformer.encoder.layers.2.self_attn.out_proj.weight: torch.Size([384, 384])\n",
      "transformer.encoder.layers.2.self_attn.out_proj.bias: torch.Size([384])\n",
      "transformer.encoder.layers.2.linear1.weight: torch.Size([2048, 384])\n",
      "transformer.encoder.layers.2.linear1.bias: torch.Size([2048])\n",
      "transformer.encoder.layers.2.linear2.weight: torch.Size([384, 2048])\n",
      "transformer.encoder.layers.2.linear2.bias: torch.Size([384])\n",
      "transformer.encoder.layers.2.conv.1.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.2.conv.1.bias: torch.Size([512])\n",
      "transformer.encoder.layers.2.conv.5.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.2.conv.5.bias: torch.Size([512])\n",
      "transformer.encoder.layers.2.conv.9.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.2.conv.9.bias: torch.Size([512])\n",
      "transformer.encoder.layers.2.fuse_cnn2trans.0.weight: torch.Size([384, 896])\n",
      "transformer.encoder.layers.2.fuse_cnn2trans.0.bias: torch.Size([384])\n",
      "transformer.encoder.layers.2.fuse_trans2cnn.0.weight: torch.Size([512, 896, 1, 1])\n",
      "transformer.encoder.layers.2.fuse_trans2cnn.0.bias: torch.Size([512])\n",
      "transformer.encoder.layers.2.norm1.weight: torch.Size([384])\n",
      "transformer.encoder.layers.2.norm1.bias: torch.Size([384])\n",
      "transformer.encoder.layers.2.norm2.weight: torch.Size([384])\n",
      "transformer.encoder.layers.2.norm2.bias: torch.Size([384])\n",
      "transformer.encoder.layers.3.self_attn.in_proj_weight: torch.Size([1152, 384])\n",
      "transformer.encoder.layers.3.self_attn.in_proj_bias: torch.Size([1152])\n",
      "transformer.encoder.layers.3.self_attn.bias_k: torch.Size([1, 1, 384])\n",
      "transformer.encoder.layers.3.self_attn.bias_v: torch.Size([1, 1, 384])\n",
      "transformer.encoder.layers.3.self_attn.out_proj.weight: torch.Size([384, 384])\n",
      "transformer.encoder.layers.3.self_attn.out_proj.bias: torch.Size([384])\n",
      "transformer.encoder.layers.3.linear1.weight: torch.Size([2048, 384])\n",
      "transformer.encoder.layers.3.linear1.bias: torch.Size([2048])\n",
      "transformer.encoder.layers.3.linear2.weight: torch.Size([384, 2048])\n",
      "transformer.encoder.layers.3.linear2.bias: torch.Size([384])\n",
      "transformer.encoder.layers.3.conv.1.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.3.conv.1.bias: torch.Size([512])\n",
      "transformer.encoder.layers.3.conv.5.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.3.conv.5.bias: torch.Size([512])\n",
      "transformer.encoder.layers.3.conv.9.weight: torch.Size([512, 512, 3, 3])\n",
      "transformer.encoder.layers.3.conv.9.bias: torch.Size([512])\n",
      "transformer.encoder.layers.3.fuse_cnn2trans.0.weight: torch.Size([384, 896])\n",
      "transformer.encoder.layers.3.fuse_cnn2trans.0.bias: torch.Size([384])\n",
      "transformer.encoder.layers.3.fuse_trans2cnn.0.weight: torch.Size([512, 896, 1, 1])\n",
      "transformer.encoder.layers.3.fuse_trans2cnn.0.bias: torch.Size([512])\n",
      "transformer.encoder.layers.3.norm1.weight: torch.Size([384])\n",
      "transformer.encoder.layers.3.norm1.bias: torch.Size([384])\n",
      "transformer.encoder.layers.3.norm2.weight: torch.Size([384])\n",
      "transformer.encoder.layers.3.norm2.bias: torch.Size([384])\n",
      "transformer.encoder.project2cnn.weight: torch.Size([512, 2048, 1, 1])\n",
      "transformer.encoder.project2cnn.bias: torch.Size([512])\n",
      "transformer.encoder.project2trans.weight: torch.Size([384, 2048])\n",
      "transformer.encoder.project2trans.bias: torch.Size([384])\n",
      "transformer.encoder.interlayer_request.self_attn.in_proj_weight: torch.Size([1152, 384])\n",
      "transformer.encoder.interlayer_request.self_attn.in_proj_bias: torch.Size([1152])\n",
      "transformer.encoder.interlayer_request.self_attn.bias_k: torch.Size([1, 1, 384])\n",
      "transformer.encoder.interlayer_request.self_attn.bias_v: torch.Size([1, 1, 384])\n",
      "transformer.encoder.interlayer_request.self_attn.out_proj.weight: torch.Size([384, 384])\n",
      "transformer.encoder.interlayer_request.self_attn.out_proj.bias: torch.Size([384])\n",
      "transformer.encoder.interlayer_request.linear1.weight: torch.Size([2048, 384])\n",
      "transformer.encoder.interlayer_request.linear1.bias: torch.Size([2048])\n",
      "transformer.encoder.interlayer_request.linear2.weight: torch.Size([384, 2048])\n",
      "transformer.encoder.interlayer_request.linear2.bias: torch.Size([384])\n",
      "transformer.encoder.interlayer_request.norm1.weight: torch.Size([384])\n",
      "transformer.encoder.interlayer_request.norm1.bias: torch.Size([384])\n",
      "transformer.encoder.interlayer_request.norm2.weight: torch.Size([384])\n",
      "transformer.encoder.interlayer_request.norm2.bias: torch.Size([384])\n",
      "backbone.0.body.conv1.weight: torch.Size([64, 3, 7, 7])\n",
      "backbone.0.body.bn1.weight: torch.Size([64])\n",
      "backbone.0.body.bn1.bias: torch.Size([64])\n",
      "backbone.0.body.bn1.running_mean: torch.Size([64])\n",
      "backbone.0.body.bn1.running_var: torch.Size([64])\n",
      "backbone.0.body.layer1.0.conv1.weight: torch.Size([64, 64, 1, 1])\n",
      "backbone.0.body.layer1.0.bn1.weight: torch.Size([64])\n",
      "backbone.0.body.layer1.0.bn1.bias: torch.Size([64])\n",
      "backbone.0.body.layer1.0.bn1.running_mean: torch.Size([64])\n",
      "backbone.0.body.layer1.0.bn1.running_var: torch.Size([64])\n",
      "backbone.0.body.layer1.0.conv2.weight: torch.Size([64, 64, 3, 3])\n",
      "backbone.0.body.layer1.0.bn2.weight: torch.Size([64])\n",
      "backbone.0.body.layer1.0.bn2.bias: torch.Size([64])\n",
      "backbone.0.body.layer1.0.bn2.running_mean: torch.Size([64])\n",
      "backbone.0.body.layer1.0.bn2.running_var: torch.Size([64])\n",
      "backbone.0.body.layer1.0.conv3.weight: torch.Size([256, 64, 1, 1])\n",
      "backbone.0.body.layer1.0.bn3.weight: torch.Size([256])\n",
      "backbone.0.body.layer1.0.bn3.bias: torch.Size([256])\n",
      "backbone.0.body.layer1.0.bn3.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer1.0.bn3.running_var: torch.Size([256])\n",
      "backbone.0.body.layer1.0.downsample.0.weight: torch.Size([256, 64, 1, 1])\n",
      "backbone.0.body.layer1.0.downsample.1.weight: torch.Size([256])\n",
      "backbone.0.body.layer1.0.downsample.1.bias: torch.Size([256])\n",
      "backbone.0.body.layer1.0.downsample.1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer1.0.downsample.1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer1.1.conv1.weight: torch.Size([64, 256, 1, 1])\n",
      "backbone.0.body.layer1.1.bn1.weight: torch.Size([64])\n",
      "backbone.0.body.layer1.1.bn1.bias: torch.Size([64])\n",
      "backbone.0.body.layer1.1.bn1.running_mean: torch.Size([64])\n",
      "backbone.0.body.layer1.1.bn1.running_var: torch.Size([64])\n",
      "backbone.0.body.layer1.1.conv2.weight: torch.Size([64, 64, 3, 3])\n",
      "backbone.0.body.layer1.1.bn2.weight: torch.Size([64])\n",
      "backbone.0.body.layer1.1.bn2.bias: torch.Size([64])\n",
      "backbone.0.body.layer1.1.bn2.running_mean: torch.Size([64])\n",
      "backbone.0.body.layer1.1.bn2.running_var: torch.Size([64])\n",
      "backbone.0.body.layer1.1.conv3.weight: torch.Size([256, 64, 1, 1])\n",
      "backbone.0.body.layer1.1.bn3.weight: torch.Size([256])\n",
      "backbone.0.body.layer1.1.bn3.bias: torch.Size([256])\n",
      "backbone.0.body.layer1.1.bn3.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer1.1.bn3.running_var: torch.Size([256])\n",
      "backbone.0.body.layer1.2.conv1.weight: torch.Size([64, 256, 1, 1])\n",
      "backbone.0.body.layer1.2.bn1.weight: torch.Size([64])\n",
      "backbone.0.body.layer1.2.bn1.bias: torch.Size([64])\n",
      "backbone.0.body.layer1.2.bn1.running_mean: torch.Size([64])\n",
      "backbone.0.body.layer1.2.bn1.running_var: torch.Size([64])\n",
      "backbone.0.body.layer1.2.conv2.weight: torch.Size([64, 64, 3, 3])\n",
      "backbone.0.body.layer1.2.bn2.weight: torch.Size([64])\n",
      "backbone.0.body.layer1.2.bn2.bias: torch.Size([64])\n",
      "backbone.0.body.layer1.2.bn2.running_mean: torch.Size([64])\n",
      "backbone.0.body.layer1.2.bn2.running_var: torch.Size([64])\n",
      "backbone.0.body.layer1.2.conv3.weight: torch.Size([256, 64, 1, 1])\n",
      "backbone.0.body.layer1.2.bn3.weight: torch.Size([256])\n",
      "backbone.0.body.layer1.2.bn3.bias: torch.Size([256])\n",
      "backbone.0.body.layer1.2.bn3.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer1.2.bn3.running_var: torch.Size([256])\n",
      "backbone.0.body.layer2.0.conv1.weight: torch.Size([128, 256, 1, 1])\n",
      "backbone.0.body.layer2.0.bn1.weight: torch.Size([128])\n",
      "backbone.0.body.layer2.0.bn1.bias: torch.Size([128])\n",
      "backbone.0.body.layer2.0.bn1.running_mean: torch.Size([128])\n",
      "backbone.0.body.layer2.0.bn1.running_var: torch.Size([128])\n",
      "backbone.0.body.layer2.0.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "backbone.0.body.layer2.0.bn2.weight: torch.Size([128])\n",
      "backbone.0.body.layer2.0.bn2.bias: torch.Size([128])\n",
      "backbone.0.body.layer2.0.bn2.running_mean: torch.Size([128])\n",
      "backbone.0.body.layer2.0.bn2.running_var: torch.Size([128])\n",
      "backbone.0.body.layer2.0.conv3.weight: torch.Size([512, 128, 1, 1])\n",
      "backbone.0.body.layer2.0.bn3.weight: torch.Size([512])\n",
      "backbone.0.body.layer2.0.bn3.bias: torch.Size([512])\n",
      "backbone.0.body.layer2.0.bn3.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer2.0.bn3.running_var: torch.Size([512])\n",
      "backbone.0.body.layer2.0.downsample.0.weight: torch.Size([512, 256, 1, 1])\n",
      "backbone.0.body.layer2.0.downsample.1.weight: torch.Size([512])\n",
      "backbone.0.body.layer2.0.downsample.1.bias: torch.Size([512])\n",
      "backbone.0.body.layer2.0.downsample.1.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer2.0.downsample.1.running_var: torch.Size([512])\n",
      "backbone.0.body.layer2.1.conv1.weight: torch.Size([128, 512, 1, 1])\n",
      "backbone.0.body.layer2.1.bn1.weight: torch.Size([128])\n",
      "backbone.0.body.layer2.1.bn1.bias: torch.Size([128])\n",
      "backbone.0.body.layer2.1.bn1.running_mean: torch.Size([128])\n",
      "backbone.0.body.layer2.1.bn1.running_var: torch.Size([128])\n",
      "backbone.0.body.layer2.1.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "backbone.0.body.layer2.1.bn2.weight: torch.Size([128])\n",
      "backbone.0.body.layer2.1.bn2.bias: torch.Size([128])\n",
      "backbone.0.body.layer2.1.bn2.running_mean: torch.Size([128])\n",
      "backbone.0.body.layer2.1.bn2.running_var: torch.Size([128])\n",
      "backbone.0.body.layer2.1.conv3.weight: torch.Size([512, 128, 1, 1])\n",
      "backbone.0.body.layer2.1.bn3.weight: torch.Size([512])\n",
      "backbone.0.body.layer2.1.bn3.bias: torch.Size([512])\n",
      "backbone.0.body.layer2.1.bn3.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer2.1.bn3.running_var: torch.Size([512])\n",
      "backbone.0.body.layer2.2.conv1.weight: torch.Size([128, 512, 1, 1])\n",
      "backbone.0.body.layer2.2.bn1.weight: torch.Size([128])\n",
      "backbone.0.body.layer2.2.bn1.bias: torch.Size([128])\n",
      "backbone.0.body.layer2.2.bn1.running_mean: torch.Size([128])\n",
      "backbone.0.body.layer2.2.bn1.running_var: torch.Size([128])\n",
      "backbone.0.body.layer2.2.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "backbone.0.body.layer2.2.bn2.weight: torch.Size([128])\n",
      "backbone.0.body.layer2.2.bn2.bias: torch.Size([128])\n",
      "backbone.0.body.layer2.2.bn2.running_mean: torch.Size([128])\n",
      "backbone.0.body.layer2.2.bn2.running_var: torch.Size([128])\n",
      "backbone.0.body.layer2.2.conv3.weight: torch.Size([512, 128, 1, 1])\n",
      "backbone.0.body.layer2.2.bn3.weight: torch.Size([512])\n",
      "backbone.0.body.layer2.2.bn3.bias: torch.Size([512])\n",
      "backbone.0.body.layer2.2.bn3.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer2.2.bn3.running_var: torch.Size([512])\n",
      "backbone.0.body.layer2.3.conv1.weight: torch.Size([128, 512, 1, 1])\n",
      "backbone.0.body.layer2.3.bn1.weight: torch.Size([128])\n",
      "backbone.0.body.layer2.3.bn1.bias: torch.Size([128])\n",
      "backbone.0.body.layer2.3.bn1.running_mean: torch.Size([128])\n",
      "backbone.0.body.layer2.3.bn1.running_var: torch.Size([128])\n",
      "backbone.0.body.layer2.3.conv2.weight: torch.Size([128, 128, 3, 3])\n",
      "backbone.0.body.layer2.3.bn2.weight: torch.Size([128])\n",
      "backbone.0.body.layer2.3.bn2.bias: torch.Size([128])\n",
      "backbone.0.body.layer2.3.bn2.running_mean: torch.Size([128])\n",
      "backbone.0.body.layer2.3.bn2.running_var: torch.Size([128])\n",
      "backbone.0.body.layer2.3.conv3.weight: torch.Size([512, 128, 1, 1])\n",
      "backbone.0.body.layer2.3.bn3.weight: torch.Size([512])\n",
      "backbone.0.body.layer2.3.bn3.bias: torch.Size([512])\n",
      "backbone.0.body.layer2.3.bn3.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer2.3.bn3.running_var: torch.Size([512])\n",
      "backbone.0.body.layer3.0.conv1.weight: torch.Size([256, 512, 1, 1])\n",
      "backbone.0.body.layer3.0.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.0.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.0.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.0.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.0.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.0.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.0.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.0.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.0.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.0.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.0.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.0.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.0.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.0.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.0.downsample.0.weight: torch.Size([1024, 512, 1, 1])\n",
      "backbone.0.body.layer3.0.downsample.1.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.0.downsample.1.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.0.downsample.1.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.0.downsample.1.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.1.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.1.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.1.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.1.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.1.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.1.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.1.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.1.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.1.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.1.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.1.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.1.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.1.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.1.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.1.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.2.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.2.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.2.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.2.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.2.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.2.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.2.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.2.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.2.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.2.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.2.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.2.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.2.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.2.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.2.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.3.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.3.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.3.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.3.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.3.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.3.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.3.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.3.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.3.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.3.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.3.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.3.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.3.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.3.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.3.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.4.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.4.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.4.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.4.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.4.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.4.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.4.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.4.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.4.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.4.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.4.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.4.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.4.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.4.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.4.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.5.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.5.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.5.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.5.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.5.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.5.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.5.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.5.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.5.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.5.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.5.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.5.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.5.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.5.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.5.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.6.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.6.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.6.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.6.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.6.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.6.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.6.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.6.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.6.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.6.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.6.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.6.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.6.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.6.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.6.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.7.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.7.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.7.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.7.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.7.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.7.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.7.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.7.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.7.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.7.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.7.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.7.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.7.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.7.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.7.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.8.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.8.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.8.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.8.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.8.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.8.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.8.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.8.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.8.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.8.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.8.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.8.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.8.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.8.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.8.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.9.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.9.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.9.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.9.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.9.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.9.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.9.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.9.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.9.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.9.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.9.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.9.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.9.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.9.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.9.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.10.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.10.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.10.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.10.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.10.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.10.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.10.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.10.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.10.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.10.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.10.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.10.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.10.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.10.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.10.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.11.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.11.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.11.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.11.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.11.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.11.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.11.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.11.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.11.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.11.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.11.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.11.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.11.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.11.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.11.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.12.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.12.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.12.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.12.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.12.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.12.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.12.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.12.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.12.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.12.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.12.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.12.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.12.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.12.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.12.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.13.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.13.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.13.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.13.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.13.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.13.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.13.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.13.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.13.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.13.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.13.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.13.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.13.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.13.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.13.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.14.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.14.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.14.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.14.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.14.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.14.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.14.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.14.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.14.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.14.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.14.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.14.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.14.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.14.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.14.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.15.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.15.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.15.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.15.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.15.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.15.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.15.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.15.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.15.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.15.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.15.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.15.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.15.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.15.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.15.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.16.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.16.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.16.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.16.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.16.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.16.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.16.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.16.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.16.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.16.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.16.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.16.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.16.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.16.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.16.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.17.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.17.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.17.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.17.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.17.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.17.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.17.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.17.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.17.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.17.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.17.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.17.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.17.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.17.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.17.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.18.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.18.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.18.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.18.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.18.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.18.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.18.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.18.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.18.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.18.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.18.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.18.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.18.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.18.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.18.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.19.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.19.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.19.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.19.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.19.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.19.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.19.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.19.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.19.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.19.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.19.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.19.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.19.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.19.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.19.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.20.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.20.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.20.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.20.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.20.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.20.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.20.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.20.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.20.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.20.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.20.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.20.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.20.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.20.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.20.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.21.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.21.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.21.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.21.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.21.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.21.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.21.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.21.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.21.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.21.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.21.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.21.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.21.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.21.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.21.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer3.22.conv1.weight: torch.Size([256, 1024, 1, 1])\n",
      "backbone.0.body.layer3.22.bn1.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.22.bn1.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.22.bn1.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.22.bn1.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.22.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "backbone.0.body.layer3.22.bn2.weight: torch.Size([256])\n",
      "backbone.0.body.layer3.22.bn2.bias: torch.Size([256])\n",
      "backbone.0.body.layer3.22.bn2.running_mean: torch.Size([256])\n",
      "backbone.0.body.layer3.22.bn2.running_var: torch.Size([256])\n",
      "backbone.0.body.layer3.22.conv3.weight: torch.Size([1024, 256, 1, 1])\n",
      "backbone.0.body.layer3.22.bn3.weight: torch.Size([1024])\n",
      "backbone.0.body.layer3.22.bn3.bias: torch.Size([1024])\n",
      "backbone.0.body.layer3.22.bn3.running_mean: torch.Size([1024])\n",
      "backbone.0.body.layer3.22.bn3.running_var: torch.Size([1024])\n",
      "backbone.0.body.layer4.0.conv1.weight: torch.Size([512, 1024, 1, 1])\n",
      "backbone.0.body.layer4.0.bn1.weight: torch.Size([512])\n",
      "backbone.0.body.layer4.0.bn1.bias: torch.Size([512])\n",
      "backbone.0.body.layer4.0.bn1.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer4.0.bn1.running_var: torch.Size([512])\n",
      "backbone.0.body.layer4.0.conv2.weight: torch.Size([512, 512, 3, 3])\n",
      "backbone.0.body.layer4.0.bn2.weight: torch.Size([512])\n",
      "backbone.0.body.layer4.0.bn2.bias: torch.Size([512])\n",
      "backbone.0.body.layer4.0.bn2.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer4.0.bn2.running_var: torch.Size([512])\n",
      "backbone.0.body.layer4.0.conv3.weight: torch.Size([2048, 512, 1, 1])\n",
      "backbone.0.body.layer4.0.bn3.weight: torch.Size([2048])\n",
      "backbone.0.body.layer4.0.bn3.bias: torch.Size([2048])\n",
      "backbone.0.body.layer4.0.bn3.running_mean: torch.Size([2048])\n",
      "backbone.0.body.layer4.0.bn3.running_var: torch.Size([2048])\n",
      "backbone.0.body.layer4.0.downsample.0.weight: torch.Size([2048, 1024, 1, 1])\n",
      "backbone.0.body.layer4.0.downsample.1.weight: torch.Size([2048])\n",
      "backbone.0.body.layer4.0.downsample.1.bias: torch.Size([2048])\n",
      "backbone.0.body.layer4.0.downsample.1.running_mean: torch.Size([2048])\n",
      "backbone.0.body.layer4.0.downsample.1.running_var: torch.Size([2048])\n",
      "backbone.0.body.layer4.1.conv1.weight: torch.Size([512, 2048, 1, 1])\n",
      "backbone.0.body.layer4.1.bn1.weight: torch.Size([512])\n",
      "backbone.0.body.layer4.1.bn1.bias: torch.Size([512])\n",
      "backbone.0.body.layer4.1.bn1.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer4.1.bn1.running_var: torch.Size([512])\n",
      "backbone.0.body.layer4.1.conv2.weight: torch.Size([512, 512, 3, 3])\n",
      "backbone.0.body.layer4.1.bn2.weight: torch.Size([512])\n",
      "backbone.0.body.layer4.1.bn2.bias: torch.Size([512])\n",
      "backbone.0.body.layer4.1.bn2.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer4.1.bn2.running_var: torch.Size([512])\n",
      "backbone.0.body.layer4.1.conv3.weight: torch.Size([2048, 512, 1, 1])\n",
      "backbone.0.body.layer4.1.bn3.weight: torch.Size([2048])\n",
      "backbone.0.body.layer4.1.bn3.bias: torch.Size([2048])\n",
      "backbone.0.body.layer4.1.bn3.running_mean: torch.Size([2048])\n",
      "backbone.0.body.layer4.1.bn3.running_var: torch.Size([2048])\n",
      "backbone.0.body.layer4.2.conv1.weight: torch.Size([512, 2048, 1, 1])\n",
      "backbone.0.body.layer4.2.bn1.weight: torch.Size([512])\n",
      "backbone.0.body.layer4.2.bn1.bias: torch.Size([512])\n",
      "backbone.0.body.layer4.2.bn1.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer4.2.bn1.running_var: torch.Size([512])\n",
      "backbone.0.body.layer4.2.conv2.weight: torch.Size([512, 512, 3, 3])\n",
      "backbone.0.body.layer4.2.bn2.weight: torch.Size([512])\n",
      "backbone.0.body.layer4.2.bn2.bias: torch.Size([512])\n",
      "backbone.0.body.layer4.2.bn2.running_mean: torch.Size([512])\n",
      "backbone.0.body.layer4.2.bn2.running_var: torch.Size([512])\n",
      "backbone.0.body.layer4.2.conv3.weight: torch.Size([2048, 512, 1, 1])\n",
      "backbone.0.body.layer4.2.bn3.weight: torch.Size([2048])\n",
      "backbone.0.body.layer4.2.bn3.bias: torch.Size([2048])\n",
      "backbone.0.body.layer4.2.bn3.running_mean: torch.Size([2048])\n",
      "backbone.0.body.layer4.2.bn3.running_var: torch.Size([2048])\n",
      "backbone.0.body.fc.weight: torch.Size([1000, 2048])\n",
      "backbone.0.body.fc.bias: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "#state_dict_decoder['model']  # 查看模型的参数字典\n",
    "# 打印所有参数的名称和形状\n",
    "\n",
    "# 遍历并打印每个参数的名称和形状\n",
    "for param_name, param_tensor in state_dict_encoder[\"model\"].items():\n",
    "    print(f\"{param_name}: {param_tensor.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925b2272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.vc_layer0.self_attn.in_proj_weight: torch.Size([1152, 384])\n",
      "decoder.vc_layer0.self_attn.in_proj_bias: torch.Size([1152])\n",
      "decoder.vc_layer0.self_attn.bias_k: torch.Size([1, 1, 384])\n",
      "decoder.vc_layer0.self_attn.bias_v: torch.Size([1, 1, 384])\n",
      "decoder.vc_layer0.self_attn.out_proj.weight: torch.Size([384, 384])\n",
      "decoder.vc_layer0.self_attn.out_proj.bias: torch.Size([384])\n",
      "decoder.vc_layer0.linear1.weight: torch.Size([2048, 384])\n",
      "decoder.vc_layer0.linear1.bias: torch.Size([2048])\n",
      "decoder.vc_layer0.linear2.weight: torch.Size([384, 2048])\n",
      "decoder.vc_layer0.linear2.bias: torch.Size([384])\n",
      "decoder.vc_layer0.norm1.weight: torch.Size([384])\n",
      "decoder.vc_layer0.norm1.bias: torch.Size([384])\n",
      "decoder.vc_layer0.norm2.weight: torch.Size([384])\n",
      "decoder.vc_layer0.norm2.bias: torch.Size([384])\n",
      "decoder.vc_layer1.self_attn.in_proj_weight: torch.Size([1152, 384])\n",
      "decoder.vc_layer1.self_attn.in_proj_bias: torch.Size([1152])\n",
      "decoder.vc_layer1.self_attn.bias_k: torch.Size([1, 1, 384])\n",
      "decoder.vc_layer1.self_attn.bias_v: torch.Size([1, 1, 384])\n",
      "decoder.vc_layer1.self_attn.out_proj.weight: torch.Size([384, 384])\n",
      "decoder.vc_layer1.self_attn.out_proj.bias: torch.Size([384])\n",
      "decoder.vc_layer1.linear1.weight: torch.Size([2048, 384])\n",
      "decoder.vc_layer1.linear1.bias: torch.Size([2048])\n",
      "decoder.vc_layer1.linear2.weight: torch.Size([384, 2048])\n",
      "decoder.vc_layer1.linear2.bias: torch.Size([384])\n",
      "decoder.vc_layer1.norm1.weight: torch.Size([384])\n",
      "decoder.vc_layer1.norm1.bias: torch.Size([384])\n",
      "decoder.vc_layer1.norm2.weight: torch.Size([384])\n",
      "decoder.vc_layer1.norm2.bias: torch.Size([384])\n",
      "decoder.vc_layer2.self_attn.in_proj_weight: torch.Size([1152, 384])\n",
      "decoder.vc_layer2.self_attn.in_proj_bias: torch.Size([1152])\n",
      "decoder.vc_layer2.self_attn.bias_k: torch.Size([1, 1, 384])\n",
      "decoder.vc_layer2.self_attn.bias_v: torch.Size([1, 1, 384])\n",
      "decoder.vc_layer2.self_attn.out_proj.weight: torch.Size([384, 384])\n",
      "decoder.vc_layer2.self_attn.out_proj.bias: torch.Size([384])\n",
      "decoder.vc_layer2.linear1.weight: torch.Size([2048, 384])\n",
      "decoder.vc_layer2.linear1.bias: torch.Size([2048])\n",
      "decoder.vc_layer2.linear2.weight: torch.Size([384, 2048])\n",
      "decoder.vc_layer2.linear2.bias: torch.Size([384])\n",
      "decoder.vc_layer2.norm1.weight: torch.Size([384])\n",
      "decoder.vc_layer2.norm1.bias: torch.Size([384])\n",
      "decoder.vc_layer2.norm2.weight: torch.Size([384])\n",
      "decoder.vc_layer2.norm2.bias: torch.Size([384])\n",
      "decoder.conv_start.0.weight: torch.Size([32, 7, 3, 3])\n",
      "decoder.conv_start.0.bias: torch.Size([32])\n",
      "decoder.conv_start.2.weight: torch.Size([64, 32, 3, 3])\n",
      "decoder.conv_start.2.bias: torch.Size([64])\n",
      "decoder.conv1_2.weight: torch.Size([64, 64, 3, 3])\n",
      "decoder.conv1_2.bias: torch.Size([64])\n",
      "decoder.conv1_2norm_ss.weight: torch.Size([64, 1, 1, 1])\n",
      "decoder.conv2_1.weight: torch.Size([128, 64, 3, 3])\n",
      "decoder.conv2_1.bias: torch.Size([128])\n",
      "decoder.conv2_2.weight: torch.Size([128, 128, 3, 3])\n",
      "decoder.conv2_2.bias: torch.Size([128])\n",
      "decoder.conv2_2norm_ss.weight: torch.Size([128, 1, 1, 1])\n",
      "decoder.conv3_1.weight: torch.Size([256, 128, 3, 3])\n",
      "decoder.conv3_1.bias: torch.Size([256])\n",
      "decoder.conv3_2.weight: torch.Size([256, 256, 3, 3])\n",
      "decoder.conv3_2.bias: torch.Size([256])\n",
      "decoder.conv3_3.weight: torch.Size([256, 256, 3, 3])\n",
      "decoder.conv3_3.bias: torch.Size([256])\n",
      "decoder.conv3_3norm_ss.weight: torch.Size([256, 1, 1, 1])\n",
      "decoder.conv4_1.weight: torch.Size([512, 256, 3, 3])\n",
      "decoder.conv4_1.bias: torch.Size([512])\n",
      "decoder.conv4_2.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv4_2.bias: torch.Size([512])\n",
      "decoder.conv4_3.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv4_3.bias: torch.Size([512])\n",
      "decoder.conv5_1.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv5_1.bias: torch.Size([512])\n",
      "decoder.conv5_2.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv5_2.bias: torch.Size([512])\n",
      "decoder.conv5_3.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv5_3.bias: torch.Size([512])\n",
      "decoder.conv6_1.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv6_1.bias: torch.Size([512])\n",
      "decoder.conv6_2.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv6_2.bias: torch.Size([512])\n",
      "decoder.conv6_3.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv6_3.bias: torch.Size([512])\n",
      "decoder.conv7_1.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv7_1.bias: torch.Size([512])\n",
      "decoder.conv7_2.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv7_2.bias: torch.Size([512])\n",
      "decoder.conv7_3.weight: torch.Size([512, 512, 3, 3])\n",
      "decoder.conv7_3.bias: torch.Size([512])\n",
      "decoder.conv8_1.1.weight: torch.Size([256, 512, 3, 3])\n",
      "decoder.conv8_1.1.bias: torch.Size([256])\n",
      "decoder.conv3_3_short.weight: torch.Size([256, 256, 3, 3])\n",
      "decoder.conv3_3_short.bias: torch.Size([256])\n",
      "decoder.conv8_2.weight: torch.Size([256, 256, 3, 3])\n",
      "decoder.conv8_2.bias: torch.Size([256])\n",
      "decoder.conv8_3.weight: torch.Size([256, 256, 3, 3])\n",
      "decoder.conv8_3.bias: torch.Size([256])\n",
      "decoder.conv9_1.1.weight: torch.Size([128, 256, 3, 3])\n",
      "decoder.conv9_1.1.bias: torch.Size([128])\n",
      "decoder.conv2_2_short.weight: torch.Size([128, 128, 3, 3])\n",
      "decoder.conv2_2_short.bias: torch.Size([128])\n",
      "decoder.conv9_2.weight: torch.Size([128, 128, 3, 3])\n",
      "decoder.conv9_2.bias: torch.Size([128])\n",
      "decoder.conv10_1.1.weight: torch.Size([128, 128, 3, 3])\n",
      "decoder.conv10_1.1.bias: torch.Size([128])\n",
      "decoder.conv1_2_short.weight: torch.Size([128, 64, 3, 3])\n",
      "decoder.conv1_2_short.bias: torch.Size([128])\n",
      "decoder.conv10_2.weight: torch.Size([128, 128, 3, 3])\n",
      "decoder.conv10_2.bias: torch.Size([128])\n",
      "decoder.conv10_ab.weight: torch.Size([2, 128, 1, 1])\n",
      "decoder.conv10_ab.bias: torch.Size([2])\n",
      "decoder.fuse_trans2cnn1.weight: torch.Size([512, 896, 1, 1])\n",
      "decoder.fuse_trans2cnn1.bias: torch.Size([512])\n",
      "decoder.fuse_cnn2trans1.weight: torch.Size([384, 896])\n",
      "decoder.fuse_cnn2trans1.bias: torch.Size([384])\n",
      "decoder.fuse_trans2cnn2.weight: torch.Size([512, 896, 1, 1])\n",
      "decoder.fuse_trans2cnn2.bias: torch.Size([512])\n",
      "decoder.fuse_cnn2trans2.weight: torch.Size([384, 896])\n",
      "decoder.fuse_cnn2trans2.bias: torch.Size([384])\n",
      "decoder.fuse_trans2cnn3.weight: torch.Size([512, 896, 1, 1])\n",
      "decoder.fuse_trans2cnn3.bias: torch.Size([512])\n",
      "decoder.fuse_cnn2trans3.weight: torch.Size([384, 896])\n",
      "decoder.fuse_cnn2trans3.bias: torch.Size([384])\n",
      "warper.warp_layer.theta.weight: torch.Size([256, 256])\n",
      "warper.warp_layer.theta.bias: torch.Size([256])\n",
      "warper.warp_layer.phi.weight: torch.Size([256, 256])\n",
      "warper.warp_layer.phi.bias: torch.Size([256])\n",
      "warper.warp_layer.pos_embed.weight: torch.Size([256, 384])\n",
      "warper.warp_layer.pos_embed.bias: torch.Size([256])\n",
      "warper.warp_layer.refine_net.0.weight: torch.Size([2, 514])\n",
      "warper.warp_layer.refine_net.0.bias: torch.Size([2])\n",
      "warper.warp_layer.refine_net.1.weight: torch.Size([1])\n",
      "warper.project_feature1.weight: torch.Size([128, 512, 1, 1])\n",
      "warper.project_feature1.bias: torch.Size([128])\n",
      "warper.project_feature2.weight: torch.Size([128, 256, 1, 1])\n",
      "warper.project_feature2.bias: torch.Size([128])\n",
      "warper.fuse_conv1_1.conv.0.weight: torch.Size([512, 512, 3, 3])\n",
      "warper.fuse_conv1_1.conv.0.bias: torch.Size([512])\n",
      "warper.fuse_conv1_1.conv.2.weight: torch.Size([512, 512, 3, 3])\n",
      "warper.fuse_conv1_1.conv.2.bias: torch.Size([512])\n",
      "warper.fuse_conv1_2.conv.0.weight: torch.Size([512, 512, 3, 3])\n",
      "warper.fuse_conv1_2.conv.0.bias: torch.Size([512])\n",
      "warper.fuse_conv1_2.conv.2.weight: torch.Size([512, 512, 3, 3])\n",
      "warper.fuse_conv1_2.conv.2.bias: torch.Size([512])\n",
      "warper.fuse_conv1_3.conv.0.weight: torch.Size([512, 512, 3, 3])\n",
      "warper.fuse_conv1_3.conv.0.bias: torch.Size([512])\n",
      "warper.fuse_conv1_3.conv.2.weight: torch.Size([512, 512, 3, 3])\n",
      "warper.fuse_conv1_3.conv.2.bias: torch.Size([512])\n",
      "warper.Conv_up.1.weight: torch.Size([256, 512, 3, 3])\n",
      "warper.Conv_up.1.bias: torch.Size([256])\n",
      "warper.Conv_up.2.weight: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "#state_dict_decoder['model']  # 查看模型的参数字典\n",
    "# 打印所有参数的名称和形状\n",
    "\n",
    "# 遍历并打印每个参数的名称和形状\n",
    "for param_name, param_tensor in state_dict_decoder[\"model\"].items():\n",
    "    print(f\"{param_name}: {param_tensor.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd906cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_1.0.weight: torch.Size([32, 8, 3, 3])\n",
      "conv1_1.0.bias: torch.Size([32])\n",
      "conv1_1.2.weight: torch.Size([64, 32, 3, 3])\n",
      "conv1_1.2.bias: torch.Size([64])\n",
      "conv1_2.weight: torch.Size([64, 64, 3, 3])\n",
      "conv1_2.bias: torch.Size([64])\n",
      "conv1_2norm_ss.weight: torch.Size([64, 1, 1, 1])\n",
      "conv2_1.weight: torch.Size([128, 64, 3, 3])\n",
      "conv2_1.bias: torch.Size([128])\n",
      "conv2_2.weight: torch.Size([128, 128, 3, 3])\n",
      "conv2_2.bias: torch.Size([128])\n",
      "conv2_2norm_ss.weight: torch.Size([128, 1, 1, 1])\n",
      "conv3_1.weight: torch.Size([256, 128, 3, 3])\n",
      "conv3_1.bias: torch.Size([256])\n",
      "conv3_2.weight: torch.Size([256, 256, 3, 3])\n",
      "conv3_2.bias: torch.Size([256])\n",
      "conv3_3.weight: torch.Size([256, 256, 3, 3])\n",
      "conv3_3.bias: torch.Size([256])\n",
      "conv3_3norm_ss.weight: torch.Size([256, 1, 1, 1])\n",
      "conv4_1.weight: torch.Size([512, 256, 3, 3])\n",
      "conv4_1.bias: torch.Size([512])\n",
      "conv4_2.weight: torch.Size([512, 512, 3, 3])\n",
      "conv4_2.bias: torch.Size([512])\n",
      "conv4_3.weight: torch.Size([512, 512, 3, 3])\n",
      "conv4_3.bias: torch.Size([512])\n",
      "conv5_1.weight: torch.Size([512, 512, 3, 3])\n",
      "conv5_1.bias: torch.Size([512])\n",
      "conv5_2.weight: torch.Size([512, 512, 3, 3])\n",
      "conv5_2.bias: torch.Size([512])\n",
      "conv5_3.weight: torch.Size([512, 512, 3, 3])\n",
      "conv5_3.bias: torch.Size([512])\n",
      "conv6_1.weight: torch.Size([512, 512, 3, 3])\n",
      "conv6_1.bias: torch.Size([512])\n",
      "conv6_2.weight: torch.Size([512, 512, 3, 3])\n",
      "conv6_2.bias: torch.Size([512])\n",
      "conv6_3.weight: torch.Size([512, 512, 3, 3])\n",
      "conv6_3.bias: torch.Size([512])\n",
      "conv7_1.weight: torch.Size([512, 512, 3, 3])\n",
      "conv7_1.bias: torch.Size([512])\n",
      "conv7_2.weight: torch.Size([512, 512, 3, 3])\n",
      "conv7_2.bias: torch.Size([512])\n",
      "conv7_3.weight: torch.Size([512, 512, 3, 3])\n",
      "conv7_3.bias: torch.Size([512])\n",
      "conv8_1.1.weight: torch.Size([256, 512, 3, 3])\n",
      "conv8_1.1.bias: torch.Size([256])\n",
      "conv3_3_short.weight: torch.Size([256, 256, 3, 3])\n",
      "conv3_3_short.bias: torch.Size([256])\n",
      "conv8_2.weight: torch.Size([256, 256, 3, 3])\n",
      "conv8_2.bias: torch.Size([256])\n",
      "conv8_3.weight: torch.Size([256, 256, 3, 3])\n",
      "conv8_3.bias: torch.Size([256])\n",
      "conv9_1.1.weight: torch.Size([128, 256, 3, 3])\n",
      "conv9_1.1.bias: torch.Size([128])\n",
      "conv2_2_short.weight: torch.Size([128, 128, 3, 3])\n",
      "conv2_2_short.bias: torch.Size([128])\n",
      "conv9_2.weight: torch.Size([128, 128, 3, 3])\n",
      "conv9_2.bias: torch.Size([128])\n",
      "conv10_1.1.weight: torch.Size([128, 128, 3, 3])\n",
      "conv10_1.1.bias: torch.Size([128])\n",
      "conv1_2_short.weight: torch.Size([128, 64, 3, 3])\n",
      "conv1_2_short.bias: torch.Size([128])\n",
      "conv10_2.weight: torch.Size([128, 128, 3, 3])\n",
      "conv10_2.bias: torch.Size([128])\n",
      "conv10_ab.weight: torch.Size([2, 128, 1, 1])\n",
      "conv10_ab.bias: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 遍历并打印每个参数的名称和形状\n",
    "for param_name, param_tensor in colornet.items():\n",
    "    print(f\"{param_name}: {param_tensor.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ed923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: Not a tensor, type=<class 'collections.OrderedDict'>\n",
      "epoch: Not a tensor, type=<class 'int'>\n",
      "total_iter: Not a tensor, type=<class 'int'>\n",
      "args: Not a tensor, type=<class 'argparse.Namespace'>\n"
     ]
    }
   ],
   "source": [
    "# 遍历每个张量（确保 param_tensor 是 tensor 再调用 .size()）\n",
    "for param_name, param_tensor in state_dict_decoder.items():\n",
    "    if isinstance(param_tensor, torch.Tensor):\n",
    "        print(f\"{param_name}: {param_tensor.size()}\")\n",
    "    else:\n",
    "        print(f\"{param_name}: Not a tensor, type={type(param_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23886d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
